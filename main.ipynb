{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMULATING DOCKERFILE\n",
    "\n",
    "%cd /content/\n",
    "\n",
    "!echo \"--------------------------------removing old files...--------------------------------\"\n",
    "!rm -rf /content/sample_data\n",
    "!rm -rf /content/audio-captioning-dcase\n",
    "!rm -rf /content/data\n",
    "\n",
    "!echo \"--------------------------------cloning all required repositories...--------------------------------\"\n",
    "!git clone https://github.com/moon-strider/audio-captioning-dcase\n",
    "\n",
    "!echo \"--------------------------------installing dependencies...--------------------------------\"\n",
    "!pip install -r /content/audio-captioning-dcase/wavetransformer/requirement_pip.txt\n",
    "\n",
    "!mkdir /content/data/\n",
    "!mkdir /content/data/clotho_dataset/\n",
    "\n",
    "!echo \"--------------------------------copying partial dataset from google drive...--------------------------------\"\n",
    "!cp -r /content/drive/MyDrive/dcasePart/evaluation/ /content/data/clotho_audio_files/\n",
    "\n",
    "!cp -r /content/audio-captioning-dcase/clotho-dataset/data/* /content/data\n",
    "!rm -f /content/data/data_splits_features/.dummy\n",
    "!mv /content/audio-captioning-dcase/wavetransformer/data/WT_pickles/* /content/data/WT_pickles\n",
    "\n",
    "# EMULATING DOCKERFILE\n",
    "\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "sys.path.append(\"/content/audio-captioning-dcase/wavetransformer\")\n",
    "sys.path.append(\"/content/audio-captioning-dcase/wavetransformer/wt_tools\")\n",
    "sys.path.append(\"/content/audio-captioning-dcase/clotho-dataset/tools\")\n",
    "sys.path.append(\"/content/audio-captioning-dcase/clotho-dataset\")\n",
    "\n",
    "from sys import stdout\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from librosa.feature import melspectrogram\n",
    "from aux_functions import get_amount_of_file_in_dir, create_split_data\n",
    "from wt_tools.file_io import load_numpy_object, dump_numpy_object\n",
    "from typing import MutableMapping, MutableSequence,\\\n",
    "    Any, Union, List, Dict, Tuple\n",
    "from torch import Tensor, no_grad\n",
    "from torch.nn import Module, DataParallel\n",
    "from wt_tools import file_io, printing\n",
    "from wt_tools.model import module_epoch_passing, get_model, get_device\n",
    "from data_handlers.clotho_loader import get_clotho_loader\n",
    "\n",
    "\n",
    "caps = settings_dataset = settings_features = config = features_drop = None\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    global settings_dataset\n",
    "    global settings_features\n",
    "    global config\n",
    "    global features_drop\n",
    "\n",
    "    logger.remove()\n",
    "    logger.add(stdout, format='{level} | [{time:HH:mm:ss}] {name} -- {message}.',\n",
    "                level='INFO', filter=lambda record: record['extra']['indent'] == 1)\n",
    "    logger.add(stdout, format='  {level} | [{time:HH:mm:ss}] {name} -- {message}.',\n",
    "                level='INFO', filter=lambda record: record['extra']['indent'] == 2)\n",
    "    main_logger = logger.bind(indent=1)\n",
    "\n",
    "    # placeholder for loading the data maybe\n",
    "    \n",
    "    settings_dataset = {\n",
    "        \"verbose\": True,\n",
    "        \"directories\": {\n",
    "            \"root_dir\": '/content/data',\n",
    "            \"annotations_dir\": '',\n",
    "            \"downloaded_audio_dir\": 'clotho_audio_files',\n",
    "        },\n",
    "        \"output_files\": {\n",
    "            \"dir_output\": 'data_splits',\n",
    "            \"file_name_template\": 'clotho_file_{audio_file_name}.npy',\n",
    "        },\n",
    "        \"audio\": {\n",
    "            \"sr\": 44100,\n",
    "            \"to_mono\": True,\n",
    "            \"max_abs_value\": 1.,\n",
    "        },\n",
    "        \"counters\": {\n",
    "            \"words_list_file_name\": 'WT_pickles/WT_words_list.p',\n",
    "            \"words_counter_file_name\": 'WT_pickles/words_frequencies.p',\n",
    "            \"characters_list_file_name\": 'WT_pickles/WT_characters_list.p',\n",
    "            \"characters_frequencies_file_name\": 'WT_pickles/characters_frequencies.p',\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    settings_features = {\n",
    "        \"package\": 'processes',\n",
    "        \"module\": 'features_log_mel_bands',\n",
    "        \"data_files_suffix\": '.npy',\n",
    "        \"keep_raw_audio_data\": False,\n",
    "        \"output\": {\n",
    "            \"dir_output\": '/content/data/clotho_dataset',\n",
    "        },\n",
    "        \"process\": {\n",
    "            \"sr\": 44100,\n",
    "            \"nb_fft\": 1024,\n",
    "            \"hop_size\": 512,\n",
    "            \"nb_mels\": 64,\n",
    "            \"window_function\": 'hann',\n",
    "            \"center\": True,\n",
    "            \"f_min\": .0,\n",
    "            \"f_max\": None,\n",
    "            \"htk\": False,\n",
    "            \"power\": 1.,\n",
    "            \"norm\": 1,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    config = {\n",
    "        \"dnn_training_settings\": {\n",
    "            \"model\": {\n",
    "                \"model_name\": 'wave_transformer_3',\n",
    "                \"encoder\": {\n",
    "                    \"in_channels_encoder\":  64,\n",
    "                    \"out_channels_encoder\": [16,32,64,128],\n",
    "                    \"kernel_size_encoder\": 3,\n",
    "                    \"dilation_rates_encoder\": [2,2,2,2],\n",
    "                    \"last_dim_encoder\": 128,\n",
    "                    \"beam_size\": 0,\n",
    "                },\n",
    "                \"decoder\": {\n",
    "                    \"num_layers_decoder\": 3,\n",
    "                    \"num_heads_decoder\": 4,\n",
    "                    \"n_features_decoder\": 128,\n",
    "                    \"n_hidden_decoder\": 128,\n",
    "                    \"nb_classes\": 4367,\n",
    "                    \"dropout_decoder\": .25,\n",
    "                }\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"input_field_name\": \"features\",\n",
    "                'output_field_name': 'words_ind',\n",
    "                \"load_into_memory\": False,\n",
    "                \"batch_size\": 1,\n",
    "                \"num_workers\": 30,\n",
    "                \"use_multiple_mode\": False\n",
    "            },\n",
    "        },\n",
    "        \"dirs_and_files\": {\n",
    "            \"root_dirs\": {\n",
    "                \"outputs\": '/content/audio-captioning-dcase/wavetransformer/outputs',\n",
    "                \"model_data\": '/content/audio-captioning-dcase/wavetransformer/data',\n",
    "                \"data\": '/content/data',\n",
    "            },\n",
    "            \"dataset\": {\n",
    "                \"features_dirs\": {\n",
    "                    \"output\": 'data_splits_features',\n",
    "                },\n",
    "                \"audio_dirs\": {\n",
    "                    \"downloaded\": 'clotho_audio_files',\n",
    "                    \"output\": 'data_splits_audio',\n",
    "                },\n",
    "                \"pickle_files_dir\": 'WT_pickles',\n",
    "                \"files\": {\n",
    "                    \"np_file_name_template\": 'clotho_file_{audio_file_name}_{caption_index}.npy',\n",
    "                    \"words_list_file_name\": 'WT_words_list.p',\n",
    "                    \"characters_list_file_name\": 'WT_character_list.p'\n",
    "                }\n",
    "            },\n",
    "            \"model\": {\n",
    "                \"model_dir\": 'models',\n",
    "                \"pre_trained_model_name\": 'best_model_16_3_9.pt'\n",
    "            },\n",
    "            \"logging\": {\n",
    "                \"logger_dir\": 'logging',\n",
    "                \"caption_logger_file\": 'caption.txt'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if not settings_dataset['verbose']:\n",
    "        main_logger.info('Verbose if off. Not logging messages')\n",
    "        logger.disable('__main__')\n",
    "        logger.disable('processes')\n",
    "\n",
    "    main_logger.info('Starting dataset creation')\n",
    "\n",
    "    dir_root = Path(settings_dataset['directories']['root_dir'])\n",
    "    dir_out = dir_root.joinpath(settings_dataset['output_files']['dir_output'])\n",
    "    dir_audio = Path(settings_dataset['directories']['downloaded_audio_dir'])\n",
    "\n",
    "    main_logger.info('Creating the .npy files')\n",
    "    create_split_data(dir_out, dir_audio, dir_root, \n",
    "                      settings_dataset['audio'], settings_dataset['output_files'])\n",
    "    main_logger.info('Done')\n",
    "\n",
    "    nb_files_audio = get_amount_of_file_in_dir(dir_root.joinpath(dir_audio))\n",
    "    nb_files_data = get_amount_of_file_in_dir(dir_out)\n",
    "\n",
    "    main_logger.info('Amount of audio files: {}'.format(nb_files_audio))\n",
    "    main_logger.info('Amount of data files: {}'.format(nb_files_data))\n",
    "    main_logger.info('Amount of data files per audio: {}'.format(nb_files_data / nb_files_audio))\n",
    "\n",
    "    main_logger.info('Done')\n",
    "\n",
    "    main_logger.info('Dataset created')\n",
    "\n",
    "    dir_output = dir_root.joinpath(dir_out)\n",
    "\n",
    "    for data_file_name in filter(lambda _x: _x.suffix == settings_features['data_files_suffix'],\n",
    "                                dir_output.iterdir()):\n",
    "\n",
    "        data_file = load_numpy_object(data_file_name)\n",
    "\n",
    "        features = feature_extraction(data_file['audio_data'].item(),\n",
    "                        **settings_features['process'])\n",
    "\n",
    "        array_data = (data_file['file_name'].item(), )\n",
    "        dtypes = [('file_name', data_file['file_name'].dtype)]\n",
    "\n",
    "        if settings_features['keep_raw_audio_data']:\n",
    "            array_data += (data_file['audio_data'].item(), )\n",
    "            dtypes.append(('audio_data', data_file['audio_data'].dtype))\n",
    "\n",
    "        array_data += (features, data_file['words_ind'].item(),)\n",
    "        dtypes.extend([('features', np.ndarray), ('words_ind', data_file['words_ind'].dtype)])\n",
    "\n",
    "        main_logger.info(\"adata with features: {}\", array_data)\n",
    "\n",
    "        np_rec_array = np.rec.array([array_data], dtype=dtypes)\n",
    "\n",
    "        file_path = Path(settings_features['output']['dir_output']).joinpath(data_file_name.name)\n",
    "\n",
    "        dump_numpy_object(np_rec_array, str(file_path)) # save to var, not to file\n",
    "\n",
    "\n",
    "    main_logger.info('Features extracted')\n",
    "\n",
    "    shutil.move(\"/content/data/data_splits_features\", \"/content/data/data_splits_features_OLD\")\n",
    "    shutil.move(\"/content/data/clotho_dataset\", \"/content/data/data_splits_features\")\n",
    "\n",
    "\n",
    "def feature_extraction(audio_data: np.ndarray, sr: int, nb_fft: int,\n",
    "                    hop_size: int, nb_mels: int, f_min: float,\n",
    "                    f_max: float, htk: bool, power: float, norm: bool,\n",
    "                    window_function: str, center: bool) -> np.ndarray:\n",
    "    \"\"\"Feature extraction function.\n",
    "\n",
    "    :param audio_data: Audio signal.\n",
    "    :type audio_data: numpy.ndarray\n",
    "    :param sr: Sampling frequency.\n",
    "    :type sr: int\n",
    "    :param nb_fft: Amount of FFT points.\n",
    "    :type nb_fft: int\n",
    "    :param hop_size: Hop size in samples.\n",
    "    :type hop_size: int\n",
    "    :param nb_mels: Amount of MEL bands.\n",
    "    :type nb_mels: int\n",
    "    :param f_min: Minimum frequency in Hertz for MEL band calculation.\n",
    "    :type f_min: float\n",
    "    :param f_max: Maximum frequency in Hertz for MEL band calculation.\n",
    "    :type f_max: float|None\n",
    "    :param htk: Use the HTK Toolbox formula instead of Auditory toolkit.\n",
    "    :type htk: bool\n",
    "    :param power: Power of the magnitude.\n",
    "    :type power: float\n",
    "    :param norm: Area normalization of MEL filters.\n",
    "    :type norm: bool\n",
    "    :param window_function: Window function.\n",
    "    :type window_function: str\n",
    "    :param center: Center the frame for FFT.\n",
    "    :type center: bool\n",
    "    :return: Log mel-bands energies of shape=(t, nb_mels)\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    y = audio_data/abs(audio_data).max()\n",
    "    mel_bands = melspectrogram(\n",
    "        y=y, sr=sr, n_fft=nb_fft, hop_length=hop_size, win_length=nb_fft,\n",
    "        window=window_function, center=center, power=power, n_mels=nb_mels,\n",
    "        fmin=f_min, fmax=f_max, htk=htk, norm=norm).T\n",
    "\n",
    "    return np.log(mel_bands + np.finfo(float).eps)\n",
    "\n",
    "\n",
    "class MyDataParallel(DataParallel):\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return getattr(self.module, name)\n",
    "\n",
    "\n",
    "def _decode_outputs(predicted_outputs: MutableSequence[Tensor],\n",
    "                    model_indices_object: MutableSequence[str],\n",
    "                    file_names: MutableSequence[Path],\n",
    "                    eos_token: str,\n",
    "                    print_to_console: bool) \\\n",
    "        -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:\n",
    "    caption_logger = logger.bind(is_caption=True, indent=None)\n",
    "    main_logger = logger.bind(is_caption=False, indent=0)\n",
    "    caption_logger.info('Captions start')\n",
    "    main_logger.info('Starting decoding of captions')\n",
    "\n",
    "    captions_pred: List[Dict] = []\n",
    "    f_names: List[str] = []\n",
    "\n",
    "    for b_predictions, f_name in zip(\n",
    "            predicted_outputs, file_names):\n",
    "        print(b_predictions.float(), \"\\n\\n\", b_predictions.cpu().numpy())\n",
    "        predicted_words = b_predictions\n",
    "        predicted_caption = [model_indices_object[i.item()]\n",
    "                            for i in predicted_words]\n",
    "        \n",
    "        try:\n",
    "            predicted_caption = predicted_caption[\n",
    "                :predicted_caption.index(eos_token)]\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        predicted_caption = ' '.join(predicted_caption)\n",
    "\n",
    "        f_n = f_name.stem\n",
    "\n",
    "        if f_n not in f_names:\n",
    "            f_names.append(f_n)\n",
    "            captions_pred.append({\n",
    "                'file_name': f_n,\n",
    "                'caption_predicted': predicted_caption})\n",
    "\n",
    "        log_strings = [f'Captions for file {f_name.stem}: ',\n",
    "                    f'\\tPredicted caption: {predicted_caption}']\n",
    "\n",
    "        [caption_logger.info(log_string)\n",
    "        for log_string in log_strings]\n",
    "\n",
    "        if print_to_console:\n",
    "            [main_logger.info(log_string)\n",
    "            for log_string in log_strings]\n",
    "\n",
    "    logger.bind(is_caption=False, indent=0).info(\n",
    "        'Decoding of captions ended')\n",
    "\n",
    "    return captions_pred\n",
    "\n",
    "def _do_inference(model: Module,\n",
    "                settings_data:  MutableMapping[str, Any],\n",
    "                settings_io:  MutableMapping[str, Any],\n",
    "                model_indices_list: MutableSequence[str]) \\\n",
    "        -> None:\n",
    "    global caps\n",
    "    model.eval()\n",
    "    logger_main = logger.bind(is_caption=False, indent=1)\n",
    "\n",
    "    logger_main.info('Getting inference data')\n",
    "    inference_data = get_clotho_loader(settings_data=settings_data, settings_io=settings_io)\n",
    "\n",
    "    logger_main.info('Done')\n",
    "\n",
    "    with no_grad():\n",
    "        outputs = module_epoch_passing(\n",
    "            data=inference_data, module=model,\n",
    "            use_y=False,\n",
    "            objective=None, optimizer=None)\n",
    "    captions_pred = _decode_outputs(\n",
    "        outputs[1],\n",
    "        model_indices_object=model_indices_list,\n",
    "        file_names=outputs[3],\n",
    "        eos_token='<eos>',\n",
    "        print_to_console=False)\n",
    "\n",
    "    logger_main.info('Inference done')\n",
    "\n",
    "    caps = pd.DataFrame(captions_pred)\n",
    "    logger_main.info(\"caps: {}\", caps)\n",
    "    \n",
    "\n",
    "def _load_indices_file(settings_files: MutableMapping[str, Any]) \\\n",
    "        -> MutableSequence[str]:\n",
    "    path = Path(\n",
    "        settings_files['root_dirs']['model_data'],\n",
    "        settings_files['dataset']['pickle_files_dir'])\n",
    "    print(settings_files['root_dirs']['model_data'])\n",
    "    print(settings_files['dataset']['pickle_files_dir'])\n",
    "    indices_filename = path.joinpath(settings_files['root_dirs']['data'],\n",
    "                                        settings_files['dataset']['pickle_files_dir'],\n",
    "                                        settings_files['dataset']['files']['words_list_file_name'])\n",
    "    \n",
    "    return file_io.load_pickle_file(indices_filename)\n",
    "\n",
    "\n",
    "def method(settings: MutableMapping[str, Any]) -> None:\n",
    "    logger_main = logger.bind(is_caption=False, indent=0)\n",
    "    logger_main.info('Bootstrapping method')\n",
    "    pretty_printer = printing.get_pretty_printer()\n",
    "    logger_inner = logger.bind(is_caption=False, indent=1)\n",
    "\n",
    "    device, device_name = get_device(0)                     # IF NO GPU, IT'LL FAIL\n",
    "\n",
    "    model_dir = Path(\n",
    "        settings['dirs_and_files']['root_dirs']['outputs'],\n",
    "        settings['dirs_and_files']['model']['model_dir']\n",
    "    )\n",
    "\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger_inner.info(f'Process on {device_name}\\n')\n",
    "\n",
    "    logger_inner.info('Settings:\\n'\n",
    "                    f'{pretty_printer.pformat(settings)}\\n')\n",
    "    \n",
    "    model_indices_list = _load_indices_file(settings['dirs_and_files'])\n",
    "    logger_inner.info('Done')\n",
    "\n",
    "    model: Union[Module, None] = None\n",
    "\n",
    "    logger_main.info('Bootstrapping done')\n",
    "\n",
    "    logger_main.info('Loading model')\n",
    "    if model is None:\n",
    "        logger_inner.info('Setting up model')\n",
    "        model = get_model(\n",
    "            settings_model=settings['dnn_training_settings']['model'],\n",
    "            settings_io=settings['dirs_and_files'],\n",
    "            output_classes=settings['dnn_training_settings']['model']['decoder']['nb_classes'],\n",
    "            device=device)\n",
    "        model.to(device)\n",
    "        logger_inner.info('Model ready')\n",
    "    logger_inner.info('Starting inference')\n",
    "    _do_inference(\n",
    "        model=model,\n",
    "        settings_data=settings['dnn_training_settings']['data'],\n",
    "        settings_io=settings['dirs_and_files'],\n",
    "        model_indices_list=model_indices_list)\n",
    "    logger_inner.info('Inference done')\n",
    "\n",
    "prepare_dataset()\n",
    "method(config)\n",
    "caps.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4b1fca65a764b45acb559e482afe389d289dd599b9f8c5fd12ff5c2ea46a65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
